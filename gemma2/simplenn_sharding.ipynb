{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade kagglehub -q\n",
    "!pip install ipywidgets -q\n",
    "!pip install tensorflow-cpu -q\n",
    "!pip install tensorflow_datasets -q\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu -q\n",
    "!pip install git+https://github.com/felafax/gemma.git -q\n",
    "!pip install qax -q\n",
    "!pip install jax-lorax -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HUB_CACHE'] = '/mnt/persistent-disk/hf/'\n",
    "os.environ['HF_HOME'] = '/mnt/persistent-disk/hf/'\n",
    "!export HF_HUB_CACHE=\"/mnt/persistent-disk/hf/\"\n",
    "!export HF_HOME=\"/mnt/persistent-disk/hf/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yWaP_LPoEcoY"
   },
   "outputs": [],
   "source": [
    "# @title Python imports\n",
    "\n",
    "import enum\n",
    "import re\n",
    "import string\n",
    "import pdb\n",
    "\n",
    "# We import JAX and some related packages.\n",
    "import chex\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "from functools import partial\n",
    "\n",
    "# For LoRA\n",
    "import lorax\n",
    "\n",
    "# We will use HuggingFace's dataset, tokenizer, and model classes.\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer, default_data_collator\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "import torch\n",
    "\n",
    "# Finally, we import Gemma.\n",
    "from gemma import params as params_lib\n",
    "from gemma import sampler as sampler_lib\n",
    "from gemma import transformer as transformer_lib\n",
    "import sentencepiece as spm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
       " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
       " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
       " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
       " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
       " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
       " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
       " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VsT2o6JEcoZ"
   },
   "source": [
    "## Fine tuning the Gemma model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import flax\n",
    "from flax.traverse_util import flatten_dict\n",
    "from flax.core.meta import unbox\n",
    "\n",
    "\n",
    "def print_params(params):\n",
    "    flat_params = flatten_dict(params)    \n",
    "    for path, param in flat_params.items():\n",
    "        # Join the path components to create a string name\n",
    "        name = \"/\".join(str(x) for x in path)\n",
    "        print(f\"Name: {name}\")\n",
    "        # print(f\"Shape: {param.shape}\")\n",
    "        # print(f\"dtype: {param.dtype}\")\n",
    "        # print(f\"Value: {param}\")\n",
    "        if isinstance(param, flax.core.meta.Partitioned):\n",
    "            array = unbox(param)\n",
    "        else:\n",
    "            array = param\n",
    "        print(jax.debug.visualize_array_sharding(array))\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "from flax.linen import Module, compact\n",
    "from flax.linen.initializers import zeros_init\n",
    "from flax.linen.dtypes import promote_dtype\n",
    "from typing import Any, Callable\n",
    "\n",
    "default_kernel_init = jax.nn.initializers.lecun_normal()\n",
    "\n",
    "class LoRADense(Module):\n",
    "    features: int\n",
    "    use_bias: bool = True\n",
    "    dtype: Any = None\n",
    "    param_dtype: Any = jnp.float32\n",
    "    precision: Any = None\n",
    "    kernel_init: Callable = default_kernel_init\n",
    "    bias_init: Callable = zeros_init()\n",
    "    lora_rank: int = 8\n",
    "    lora_alpha: float = 16\n",
    "\n",
    "    @compact\n",
    "    def __call__(self, inputs: Any) -> Any:\n",
    "        kernel = self.variable(\n",
    "            'original_params', 'kernel',\n",
    "            self.kernel_init,\n",
    "            jax.random.PRNGKey(0),  # You might want to pass a proper key\n",
    "            (jnp.shape(inputs)[-1], self.features),\n",
    "            self.param_dtype\n",
    "        )\n",
    "        \n",
    "        if self.use_bias:\n",
    "            bias = self.variable(\n",
    "                'original_params', 'bias',\n",
    "                self.bias_init,\n",
    "                jax.random.PRNGKey(1),  # You might want to pass a proper key\n",
    "                (self.features,),\n",
    "                self.param_dtype\n",
    "            )\n",
    "        else:\n",
    "            bias = None\n",
    "\n",
    "        # LoRA weights (these remain as trainable parameters)\n",
    "        lora_a = self.param(\n",
    "            'lora_a',\n",
    "            default_kernel_init,\n",
    "            (jnp.shape(inputs)[-1], self.lora_rank),\n",
    "            self.param_dtype\n",
    "        )\n",
    "        lora_b = self.param(\n",
    "            'lora_b',\n",
    "            zeros_init(),\n",
    "            (self.lora_rank, self.features),\n",
    "            self.param_dtype\n",
    "        )\n",
    "\n",
    "        inputs, kernel_value, lora_a, lora_b, bias_value = promote_dtype(\n",
    "            inputs, kernel.value, lora_a, lora_b, \n",
    "            None if bias is None else bias.value, \n",
    "            dtype=self.dtype\n",
    "        )\n",
    "\n",
    "        y = lax.dot_general(\n",
    "            inputs,\n",
    "            jax.lax.stop_gradient(kernel_value),\n",
    "            (((inputs.ndim - 1,), (0,)), ((), ())),\n",
    "            precision=self.precision,\n",
    "        )\n",
    "\n",
    "        # LoRA computation\n",
    "        lora_output = lax.dot_general(\n",
    "            inputs,\n",
    "            lora_a,\n",
    "            (((inputs.ndim - 1,), (0,)), ((), ())),\n",
    "            precision=self.precision,\n",
    "        )\n",
    "        lora_output = lax.dot_general(\n",
    "            lora_output,\n",
    "            lora_b,\n",
    "            (((lora_output.ndim - 1,), (0,)), ((), ())),\n",
    "            precision=self.precision,\n",
    "        )\n",
    "        y += (self.lora_alpha / self.lora_rank) * lora_output\n",
    "\n",
    "        if bias_value is not None:\n",
    "            y += jnp.reshape(bias_value, (1,) * (y.ndim - 1) + (-1,))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 4\n",
    "hidden_dim = 2\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Module):\n",
    "    hidden_dim: int\n",
    "    output_dim: int\n",
    "\n",
    "    @compact\n",
    "    def __call__(self, x):\n",
    "        x = LoRADense(features=self.hidden_dim, \n",
    "                      kernel_init=jax.nn.initializers.xavier_normal(),\n",
    "                      use_bias=False)(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = LoRADense(features=self.output_dim, \n",
    "                      use_bias=False)(x)\n",
    "        return x\n",
    "\n",
    "# model = Model(hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "# params = model.init({\n",
    "#     'params': jax.random.PRNGKey(0), \n",
    "#     'original_params': jax.random.PRNGKey(1)\n",
    "# }, jnp.zeros((1, 8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients:\n",
      "(4, 2) 0.0\n",
      "(2, 1) 0.0\n",
      "(4, 8) 0.0\n",
      "(8, 2) -0.99083626\n",
      "(2, 8) 0.0\n",
      "(8, 1) 2.5092876\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'original_params': {'LoRADense_0': {'kernel': None},\n",
       "  'LoRADense_1': {'kernel': None}},\n",
       " 'params': {'LoRADense_0': {'lora_a': None, 'lora_b': None},\n",
       "  'LoRADense_1': {'lora_a': None, 'lora_b': None}}}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define dimensions\n",
    "input_dim = 4\n",
    "hidden_dim = 2\n",
    "output_dim = 1\n",
    "\n",
    "# Initialize model\n",
    "model = Model(hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "key = jax.random.PRNGKey(0)\n",
    "params = model.init(key, jnp.zeros((1, input_dim)))\n",
    "\n",
    "# Define batch\n",
    "batch = (jnp.ones(shape=(1, input_dim)), jnp.zeros(shape=(1, output_dim)))\n",
    "\n",
    "# Define loss function (Mean Squared Error)\n",
    "def loss_fn(params, batch):\n",
    "    inputs, targets = batch\n",
    "    outputs = model.apply(params, inputs)\n",
    "    return jnp.mean((outputs - targets) ** 2)\n",
    "\n",
    "# Define forward pass and gradient function\n",
    "forward_pass = lambda params, batch: loss_fn(params, batch)\n",
    "grad_fn = jax.grad(forward_pass)\n",
    "\n",
    "# Compute gradients\n",
    "grads = grad_fn(params, batch)\n",
    "\n",
    "# Print gradients\n",
    "print(\"Gradients:\")\n",
    "jax.tree_util.tree_map(lambda g: print(g.shape, jnp.mean(g)), grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'original_params': {'LoRADense_0': {'kernel': Array([[0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.]], dtype=float32)}, 'LoRADense_1': {'kernel': Array([[0.],\n",
      "       [0.]], dtype=float32)}}, 'params': {'LoRADense_0': {'lora_a': Array([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32), 'lora_b': Array([[ 0.       ,  4.9100347],\n",
      "       [-0.       , -5.7454195],\n",
      "       [ 0.       ,  1.7669095],\n",
      "       [-0.       , -4.0578656],\n",
      "       [-0.       , -1.7626989],\n",
      "       [-0.       , -7.924409 ],\n",
      "       [ 0.       ,  4.2498674],\n",
      "       [-0.       , -7.2897983]], dtype=float32)}, 'LoRADense_1': {'lora_a': Array([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32), 'lora_b': Array([[  8.193271 ],\n",
      "       [ 16.42541  ],\n",
      "       [ -3.104372 ],\n",
      "       [-18.169437 ],\n",
      "       [ 15.23544  ],\n",
      "       [  9.356257 ],\n",
      "       [ -8.72578  ],\n",
      "       [  0.8635128]], dtype=float32)}}}\n"
     ]
    }
   ],
   "source": [
    "print(grads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_params': {'LoRADense_0': {'kernel': Array([[ 0.04115987,  0.8384647 ],\n",
       "          [ 0.14326866,  0.53145474],\n",
       "          [ 0.16196674, -0.39632457],\n",
       "          [ 0.31242397,  0.4965637 ],\n",
       "          [-0.3758965 , -0.21959446],\n",
       "          [ 0.30370873,  0.39126053],\n",
       "          [-0.15885408, -0.53257084],\n",
       "          [-0.75438243,  0.13228753]], dtype=float32)},\n",
       "  'LoRADense_1': {'kernel': Array([[-0.59603935],\n",
       "          [ 0.6490678 ]], dtype=float32)}},\n",
       " 'params': {'LoRADense_0': {'lora_a': Array([[-0.77251935, -0.34922582, -0.16781904,  0.45006755,  0.5829724 ,\n",
       "           -0.20926498,  0.09819026,  0.41416237],\n",
       "          [-0.15905392, -0.56037694,  0.00484613,  0.04173131, -0.00559456,\n",
       "           -0.4811671 , -0.03955451,  0.02943526],\n",
       "          [-0.54997915,  0.07164625, -0.37911326,  0.311233  , -0.47716334,\n",
       "            0.16550043, -0.4135211 , -0.6538287 ],\n",
       "          [ 0.17607015,  0.15953197, -0.01787516,  0.01488424, -0.33595267,\n",
       "           -0.21633081, -0.0459558 ,  0.34950352],\n",
       "          [-0.23908016,  0.59608513, -0.10165072,  0.12569624, -0.31494457,\n",
       "            0.6722365 , -0.18727198, -0.3419378 ],\n",
       "          [ 0.19392161, -0.38738972,  0.04430149, -0.03902178,  0.34809998,\n",
       "            0.14428407,  0.10732901,  0.10583513],\n",
       "          [-0.20962286,  0.28242254, -0.09099862, -0.60378206,  0.03637806,\n",
       "           -0.14417359, -0.25875488,  0.2802286 ],\n",
       "          [-0.02781862, -0.6399045 ,  0.2871678 , -0.39200595,  0.27804643,\n",
       "            0.7205086 ,  0.06031857,  0.30605868]], dtype=float32),\n",
       "   'lora_b': Array([[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]], dtype=float32)},\n",
       "  'LoRADense_1': {'lora_a': Array([[-0.78383255, -1.0786403 ,  0.36627775,  0.33912653, -0.7523994 ,\n",
       "            0.14843002,  0.83820796,  0.3706167 ],\n",
       "          [ 0.47237086,  0.94698256, -0.17897795, -1.0475318 ,  0.8783766 ,\n",
       "            0.53942114, -0.50307184,  0.04978454]], dtype=float32),\n",
       "   'lora_b': Array([[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]], dtype=float32)}}}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.8058445]], dtype=float32)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(params, jnp.ones((1, 8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = (jnp.ones(shape=(1, input_dim)), jnp.zeros(shape=(1, output_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize function\n",
    "def init_fn(key, x, model, optimizer):\n",
    "    params = model.init(key, x)  # Initialize the model\n",
    "    state = train_state.TrainState.create(  # Create a `TrainState`\n",
    "        apply_fn=model.apply,\n",
    "        params=params['params'],\n",
    "        tx=optimizer)\n",
    "    return state\n",
    "\n",
    "def forward_pass(params, state, batch):\n",
    "  input_images, labels = batch\n",
    "    \n",
    "  # call forward pass function.\n",
    "  logits = state.apply_fn({\"params\": params}, input_images)\n",
    "\n",
    "  # compute loss\n",
    "  loss = optax.squared_error(logits, labels)\n",
    "  loss = loss.mean()\n",
    "  return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grad_fn = jax.value_and_grad(forward_pass, argnums=(0), has_aux=True)\n",
    "\n",
    "# compute gradients.\n",
    "(loss, _), grads = grad_fn(state.params, state, batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.init(jax.random.PRNGKey(99), jnp.zeros((1, 8)))['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dense_0': {'kernel': Array([[-0.49519178, -0.2765769 ,  0.29829058,  0.1825179 ,  0.12744386,\n",
       "           0.01904512,  0.5438577 , -0.5497272 ],\n",
       "         [-0.2905944 , -0.2480456 , -0.20327911, -0.75905126,  0.14114662,\n",
       "           0.4145826 , -0.19347848,  0.47221926],\n",
       "         [ 0.668468  , -0.2039723 ,  0.38946563,  0.13857874, -0.45398667,\n",
       "          -0.24390857, -0.32775238,  0.3971401 ],\n",
       "         [-0.29118022,  0.05225108,  0.40288976, -0.13084824, -0.25388727,\n",
       "          -0.36863217, -0.638132  , -0.15610406],\n",
       "         [ 0.18570937,  0.26377958, -0.02895478,  0.05723229, -0.39722154,\n",
       "          -0.2310003 ,  0.7511915 , -0.2714732 ],\n",
       "         [-0.3982755 ,  0.36451694,  0.20695516, -0.01715776, -0.44657674,\n",
       "           0.21720676,  0.5451954 ,  0.56244195],\n",
       "         [-0.2991815 ,  0.4356464 , -0.6718789 ,  0.16581155,  0.05301353,\n",
       "           0.32873207,  0.16510576, -0.34548888],\n",
       "         [-0.57636905, -0.15388115, -0.69777644,  0.33658162,  0.36329   ,\n",
       "          -0.34397233, -0.13496013, -0.6588193 ]], dtype=float32)},\n",
       " 'Dense_1': {'kernel': Array([[-0.14665851],\n",
       "         [-0.22765379],\n",
       "         [-0.4117055 ],\n",
       "         [-0.47610268],\n",
       "         [ 0.09983318],\n",
       "         [-0.55441415],\n",
       "         [-0.32388306],\n",
       "         [ 0.44042167]], dtype=float32)}}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dense_0': {'kernel': Array([[-0.49519178, -0.2765769 ,  0.29829058,  0.1825179 ,  0.12744386,\n",
       "           0.01904512,  0.5438577 , -0.5497272 ],\n",
       "         [-0.2905944 , -0.2480456 , -0.20327911, -0.75905126,  0.14114662,\n",
       "           0.4145826 , -0.19347848,  0.47221926],\n",
       "         [ 0.668468  , -0.2039723 ,  0.38946563,  0.13857874, -0.45398667,\n",
       "          -0.24390857, -0.32775238,  0.3971401 ],\n",
       "         [-0.29118022,  0.05225108,  0.40288976, -0.13084824, -0.25388727,\n",
       "          -0.36863217, -0.638132  , -0.15610406],\n",
       "         [ 0.18570937,  0.26377958, -0.02895478,  0.05723229, -0.39722154,\n",
       "          -0.2310003 ,  0.7511915 , -0.2714732 ],\n",
       "         [-0.3982755 ,  0.36451694,  0.20695516, -0.01715776, -0.44657674,\n",
       "           0.21720676,  0.5451954 ,  0.56244195],\n",
       "         [-0.2991815 ,  0.4356464 , -0.6718789 ,  0.16581155,  0.05301353,\n",
       "           0.32873207,  0.16510576, -0.34548888],\n",
       "         [-0.57636905, -0.15388115, -0.69777644,  0.33658162,  0.36329   ,\n",
       "          -0.34397233, -0.13496013, -0.6588193 ]], dtype=float32)},\n",
       " 'Dense_1': {'kernel': Array([[-0.14665851],\n",
       "         [-0.22765379],\n",
       "         [-0.4117055 ],\n",
       "         [-0.47610268],\n",
       "         [ 0.09983318],\n",
       "         [-0.55441415],\n",
       "         [-0.32388306],\n",
       "         [ 0.44042167]], dtype=float32)}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-0.28349656]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply({\"params\": params}, jnp.ones((1, 8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lora parms are in different namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "from flax import linen as nn\n",
    "from flax.core import freeze, unfreeze\n",
    "from flax.linen.initializers import zeros_init\n",
    "from flax.linen.dtypes import promote_dtype\n",
    "from typing import Any, Callable\n",
    "import optax\n",
    "\n",
    "default_kernel_init = jax.nn.initializers.lecun_normal()\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "from flax.linen import Module, compact\n",
    "from flax.linen.initializers import zeros_init\n",
    "from flax.linen.dtypes import promote_dtype\n",
    "from typing import Any, Callable\n",
    "\n",
    "default_kernel_init = jax.nn.initializers.lecun_normal()\n",
    "\n",
    "class LoRADense(Module):\n",
    "    features: int\n",
    "    use_bias: bool = True\n",
    "    dtype: Any = None\n",
    "    param_dtype: Any = jnp.float32\n",
    "    precision: Any = None\n",
    "    kernel_init: Callable = default_kernel_init\n",
    "    bias_init: Callable = zeros_init()\n",
    "    lora_rank: int = 8\n",
    "    lora_alpha: float = 16\n",
    "\n",
    "    @compact\n",
    "    def __call__(self, inputs: Any) -> Any:\n",
    "        kernel = self.variable(\n",
    "            'params', 'kernel',\n",
    "            self.kernel_init,\n",
    "            jax.random.PRNGKey(0),  # You might want to pass a proper key\n",
    "            (jnp.shape(inputs)[-1], self.features),\n",
    "            self.param_dtype\n",
    "        )\n",
    "        \n",
    "        if self.use_bias:\n",
    "            bias = self.variable(\n",
    "                'params', 'bias',\n",
    "                self.bias_init,\n",
    "                jax.random.PRNGKey(1),  # You might want to pass a proper key\n",
    "                (self.features,),\n",
    "                self.param_dtype\n",
    "            )\n",
    "        else:\n",
    "            bias = None\n",
    "\n",
    "        # LoRA weights (these remain as trainable parameters)\n",
    "\n",
    "        # LoRA weights are moved to \"lora_params\" scope but are still trainable\n",
    "        lora_a = self.variable(\n",
    "                'lora_params', 'lora_a',\n",
    "                self.bias_init,\n",
    "                jax.random.PRNGKey(1),  # You might want to pass a proper key\n",
    "                (jnp.shape(inputs)[-1], self.lora_rank),\n",
    "                self.param_dtype\n",
    "            )\n",
    "        lora_b = self.variable(\n",
    "                'lora_params', 'lora_b',\n",
    "                self.bias_init,\n",
    "                jax.random.PRNGKey(1),  # You might want to pass a proper key\n",
    "                (self.lora_rank, self.features),\n",
    "                self.param_dtype\n",
    "            )\n",
    "\n",
    "        inputs, kernel_value, lora_a, lora_b, bias_value = promote_dtype(\n",
    "            inputs, kernel.value, lora_a.value, lora_b.value, \n",
    "            None if bias is None else bias.value, \n",
    "            dtype=self.dtype\n",
    "        )\n",
    "\n",
    "        y = lax.dot_general(\n",
    "            inputs,\n",
    "            jax.lax.stop_gradient(kernel_value),\n",
    "            (((inputs.ndim - 1,), (0,)), ((), ())),\n",
    "            precision=self.precision,\n",
    "        )\n",
    "\n",
    "        # LoRA computation\n",
    "        lora_output = lax.dot_general(\n",
    "            inputs,\n",
    "            lora_a,\n",
    "            (((inputs.ndim - 1,), (0,)), ((), ())),\n",
    "            precision=self.precision,\n",
    "        )\n",
    "        lora_output = lax.dot_general(\n",
    "            lora_output,\n",
    "            lora_b,\n",
    "            (((lora_output.ndim - 1,), (0,)), ((), ())),\n",
    "            precision=self.precision,\n",
    "        )\n",
    "        y += (self.lora_alpha / self.lora_rank) * lora_output\n",
    "\n",
    "        if bias_value is not None:\n",
    "            y += jnp.reshape(bias_value, (1,) * (y.ndim - 1) + (-1,))\n",
    "        return y\n",
    "class Model(nn.Module):\n",
    "    hidden_dim: int\n",
    "    output_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = LoRADense(features=self.hidden_dim,\n",
    "                      kernel_init=jax.nn.initializers.xavier_normal(),\n",
    "                      use_bias=False)(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = LoRADense(features=self.output_dim,\n",
    "                      use_bias=False)(x)\n",
    "        return x\n",
    "\n",
    "# Define dimensions\n",
    "input_dim = 4\n",
    "hidden_dim = 2\n",
    "output_dim = 1\n",
    "\n",
    "# Initialize model\n",
    "model = Model(hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "key = jax.random.PRNGKey(0)\n",
    "variables = model.init(key, jnp.zeros((1, input_dim)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'params': {'LoRADense_0': {'kernel': Array([[ 0.0506583 , -0.24143301],\n",
      "       [-0.23484214,  0.977799  ],\n",
      "       [-0.77396363,  1.156682  ],\n",
      "       [-0.5310172 ,  0.69166714]], dtype=float32)}, 'LoRADense_1': {'kernel': Array([[-0.596048 ],\n",
      "       [ 0.6490589]], dtype=float32)}}, 'lora_params': {'LoRADense_0': {'lora_a': Array([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32), 'lora_b': Array([[0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.]], dtype=float32)}, 'LoRADense_1': {'lora_a': Array([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32), 'lora_b': Array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)}}}\n"
     ]
    }
   ],
   "source": [
    "print(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients: FrozenDict({\n",
      "    LoRADense_0: {\n",
      "        lora_a: Array([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "               [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "               [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "               [0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32),\n",
      "        lora_b: Array([[0., 0.],\n",
      "               [0., 0.],\n",
      "               [0., 0.],\n",
      "               [0., 0.],\n",
      "               [0., 0.],\n",
      "               [0., 0.],\n",
      "               [0., 0.],\n",
      "               [0., 0.]], dtype=float32),\n",
      "    },\n",
      "    LoRADense_1: {\n",
      "        lora_a: Array([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "               [0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32),\n",
      "        lora_b: Array([[0.],\n",
      "               [0.],\n",
      "               [0.],\n",
      "               [0.],\n",
      "               [0.],\n",
      "               [0.],\n",
      "               [0.],\n",
      "               [0.]], dtype=float32),\n",
      "    },\n",
      "})\n",
      "\n",
      "Optimizer State: (ScaleByAdamState(count=Array(0, dtype=int32), mu={'LoRADense_0': {'lora_a': Array([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32), 'lora_b': Array([[0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.]], dtype=float32)}, 'LoRADense_1': {'lora_a': Array([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32), 'lora_b': Array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)}}, nu={'LoRADense_0': {'lora_a': Array([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32), 'lora_b': Array([[0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.]], dtype=float32)}, 'LoRADense_1': {'lora_a': Array([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32), 'lora_b': Array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)}}), EmptyState())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Separate constants and trainable parameters\n",
    "constants, params = variables.pop('params'), variables.pop('lora_params')\n",
    "variables = freeze({'lora_params': params, 'params': constants})\n",
    "\n",
    "# Define batch\n",
    "batch = (jnp.ones(shape=(1, input_dim)), jnp.zeros(shape=(1, output_dim)))\n",
    "\n",
    "# Define loss function (Mean Squared Error)\n",
    "def loss_fn(params, constants, batch):\n",
    "    inputs, targets = batch\n",
    "    variables = {'lora_params': params, 'params': constants}\n",
    "    outputs = model.apply(variables, inputs)\n",
    "    return jnp.mean((outputs - targets) ** 2)\n",
    "\n",
    "# Define forward pass and gradient function\n",
    "forward_pass = lambda params, constants, batch: loss_fn(params, constants, batch)\n",
    "grad_fn = jax.grad(forward_pass, argnums=0)  # Only compute gradients w.r.t. params\n",
    "\n",
    "# Compute gradients\n",
    "grads = grad_fn(variables['lora_params'], variables['params'], batch)\n",
    "\n",
    "# Print gradients\n",
    "print(\"Gradients:\", grads)\n",
    "# jax.tree_util.tree_map(lambda g: print(g.shape, jnp.mean(g)), grads)\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = optax.adam(learning_rate=1e-3)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "# Print optimizer state\n",
    "print(\"\\nOptimizer State:\", opt_state)\n",
    "# jax.tree_util.tree_map(lambda x: print(type(x), x.shape if hasattr(x, 'shape') else x), opt_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients: FrozenDict({\n",
      "    LoRADense_0: {\n",
      "        lora_a: Array([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "               [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "               [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "               [0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32),\n",
      "        lora_b: Array([[ 0.       ,  4.9099245],\n",
      "               [-0.       , -5.745265 ],\n",
      "               [ 0.       ,  1.7668908],\n",
      "               [-0.       , -4.0577426],\n",
      "               [-0.       , -1.7626575],\n",
      "               [-0.       , -7.9242153],\n",
      "               [ 0.       ,  4.249761 ],\n",
      "               [-0.       , -7.2896028]], dtype=float32),\n",
      "    },\n",
      "    LoRADense_1: {\n",
      "        lora_a: Array([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "               [0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32),\n",
      "        lora_b: Array([[  8.193145  ],\n",
      "               [ 16.425367  ],\n",
      "               [ -3.1043384 ],\n",
      "               [-18.169392  ],\n",
      "               [ 15.235281  ],\n",
      "               [  9.35627   ],\n",
      "               [ -8.725613  ],\n",
      "               [  0.86350346]], dtype=float32),\n",
      "    },\n",
      "})\n",
      "\n",
      "Optimizer State: (ScaleByAdamState(count=Array(0, dtype=int32), mu={'LoRADense_0': {'lora_a': Array([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32), 'lora_b': Array([[0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.]], dtype=float32)}, 'LoRADense_1': {'lora_a': Array([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32), 'lora_b': Array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)}}, nu={'LoRADense_0': {'lora_a': Array([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32), 'lora_b': Array([[0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.]], dtype=float32)}, 'LoRADense_1': {'lora_a': Array([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32), 'lora_b': Array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32)}}), EmptyState())\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "from flax import linen as nn\n",
    "from flax.core import freeze, unfreeze\n",
    "from flax.linen.initializers import zeros_init\n",
    "from flax.linen.dtypes import promote_dtype\n",
    "from typing import Any, Callable\n",
    "import optax\n",
    "\n",
    "default_kernel_init = jax.nn.initializers.lecun_normal()\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "from flax.linen import Module, compact\n",
    "from flax.linen.initializers import zeros_init\n",
    "from flax.linen.dtypes import promote_dtype\n",
    "from typing import Any, Callable\n",
    "\n",
    "default_kernel_init = jax.nn.initializers.lecun_normal()\n",
    "\n",
    "class LoRADense(Module):\n",
    "    features: int\n",
    "    use_bias: bool = True\n",
    "    dtype: Any = None\n",
    "    param_dtype: Any = jnp.float32\n",
    "    precision: Any = None\n",
    "    kernel_init: Callable = default_kernel_init\n",
    "    bias_init: Callable = zeros_init()\n",
    "    lora_rank: int = 8\n",
    "    lora_alpha: float = 16\n",
    "\n",
    "    @compact\n",
    "    def __call__(self, inputs: Any) -> Any:\n",
    "        kernel = self.variable(\n",
    "            'constants', 'kernel',\n",
    "            self.kernel_init,\n",
    "            jax.random.PRNGKey(0),  # You might want to pass a proper key\n",
    "            (jnp.shape(inputs)[-1], self.features),\n",
    "            self.param_dtype\n",
    "        )\n",
    "        \n",
    "        if self.use_bias:\n",
    "            bias = self.variable(\n",
    "                'constants', 'bias',\n",
    "                self.bias_init,\n",
    "                jax.random.PRNGKey(1),  # You might want to pass a proper key\n",
    "                (self.features,),\n",
    "                self.param_dtype\n",
    "            )\n",
    "        else:\n",
    "            bias = None\n",
    "\n",
    "        # LoRA weights (these remain as trainable parameters)\n",
    "        lora_a = self.param(\n",
    "            'lora_a',\n",
    "            default_kernel_init,\n",
    "            (jnp.shape(inputs)[-1], self.lora_rank),\n",
    "            self.param_dtype\n",
    "        )\n",
    "        lora_b = self.param(\n",
    "            'lora_b',\n",
    "            zeros_init(),\n",
    "            (self.lora_rank, self.features),\n",
    "            self.param_dtype\n",
    "        )\n",
    "\n",
    "        inputs, kernel_value, lora_a, lora_b, bias_value = promote_dtype(\n",
    "            inputs, kernel.value, lora_a, lora_b, \n",
    "            None if bias is None else bias.value, \n",
    "            dtype=self.dtype\n",
    "        )\n",
    "\n",
    "        y = lax.dot_general(\n",
    "            inputs,\n",
    "            jax.lax.stop_gradient(kernel_value),\n",
    "            (((inputs.ndim - 1,), (0,)), ((), ())),\n",
    "            precision=self.precision,\n",
    "        )\n",
    "\n",
    "        # LoRA computation\n",
    "        lora_output = lax.dot_general(\n",
    "            inputs,\n",
    "            lora_a,\n",
    "            (((inputs.ndim - 1,), (0,)), ((), ())),\n",
    "            precision=self.precision,\n",
    "        )\n",
    "        lora_output = lax.dot_general(\n",
    "            lora_output,\n",
    "            lora_b,\n",
    "            (((lora_output.ndim - 1,), (0,)), ((), ())),\n",
    "            precision=self.precision,\n",
    "        )\n",
    "        y += (self.lora_alpha / self.lora_rank) * lora_output\n",
    "\n",
    "        if bias_value is not None:\n",
    "            y += jnp.reshape(bias_value, (1,) * (y.ndim - 1) + (-1,))\n",
    "        return y\n",
    "class Model(nn.Module):\n",
    "    hidden_dim: int\n",
    "    output_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = LoRADense(features=self.hidden_dim,\n",
    "                      kernel_init=jax.nn.initializers.xavier_normal(),\n",
    "                      use_bias=False)(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = LoRADense(features=self.output_dim,\n",
    "                      use_bias=False)(x)\n",
    "        return x\n",
    "\n",
    "# Define dimensions\n",
    "input_dim = 4\n",
    "hidden_dim = 2\n",
    "output_dim = 1\n",
    "\n",
    "# Initialize model\n",
    "model = Model(hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "key = jax.random.PRNGKey(0)\n",
    "variables = model.init(key, jnp.zeros((1, input_dim)))\n",
    "\n",
    "# Separate constants and trainable parameters\n",
    "constants, params = variables.pop('constants'), variables.pop('params')\n",
    "variables = freeze({'params': params, 'constants': constants})\n",
    "\n",
    "# Define batch\n",
    "batch = (jnp.ones(shape=(1, input_dim)), jnp.zeros(shape=(1, output_dim)))\n",
    "\n",
    "# Define loss function (Mean Squared Error)\n",
    "def loss_fn(params, constants, batch):\n",
    "    inputs, targets = batch\n",
    "    variables = {'params': params, 'constants': constants}\n",
    "    outputs = model.apply(variables, inputs)\n",
    "    return jnp.mean((outputs - targets) ** 2)\n",
    "\n",
    "# Define forward pass and gradient function\n",
    "forward_pass = lambda params, constants, batch: loss_fn(params, constants, batch)\n",
    "grad_fn = jax.grad(forward_pass, argnums=0)  # Only compute gradients w.r.t. params\n",
    "\n",
    "# Compute gradients\n",
    "grads = grad_fn(variables['params'], variables['constants'], batch)\n",
    "\n",
    "# Print gradients\n",
    "print(\"Gradients:\", grads)\n",
    "# jax.tree_util.tree_map(lambda g: print(g.shape, jnp.mean(g)), grads)\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = optax.adam(learning_rate=1e-3)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "# Print optimizer state\n",
    "print(\"\\nOptimizer State:\", opt_state)\n",
    "# jax.tree_util.tree_map(lambda x: print(type(x), x.shape if hasattr(x, 'shape') else x), opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients: {'params': {'Dense_0': {'kernel': Array([[0.       , 0.0030845],\n",
      "       [0.       , 0.0030845],\n",
      "       [0.       , 0.0030845],\n",
      "       [0.       , 0.0030845]], dtype=float32)}, 'Dense_1': {'kernel': Array([[ 0.        ],\n",
      "       [-0.02614744]], dtype=float32)}}}\n",
      "\n",
      "Optimizer State: (ScaleByAdamState(count=Array(0, dtype=int32), mu={'params': {'Dense_0': {'kernel': Array([[0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.]], dtype=float32)}, 'Dense_1': {'kernel': Array([[0.],\n",
      "       [0.]], dtype=float32)}}}, nu={'params': {'Dense_0': {'kernel': Array([[0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.]], dtype=float32)}, 'Dense_1': {'kernel': Array([[0.],\n",
      "       [0.]], dtype=float32)}}}), EmptyState())\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.core import freeze, unfreeze\n",
    "import optax\n",
    "\n",
    "class Model(nn.Module):\n",
    "    hidden_dim: int\n",
    "    output_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(features=self.hidden_dim,\n",
    "                     kernel_init=jax.nn.initializers.xavier_normal(),\n",
    "                     use_bias=False)(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = nn.Dense(features=self.output_dim,\n",
    "                     use_bias=False)(x)\n",
    "        return x\n",
    "\n",
    "# Define dimensions\n",
    "input_dim = 4\n",
    "hidden_dim = 2\n",
    "output_dim = 1\n",
    "\n",
    "# Initialize model\n",
    "model = Model(hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "key = jax.random.PRNGKey(0)\n",
    "params = model.init(key, jnp.zeros((1, input_dim)))\n",
    "\n",
    "# Define batch\n",
    "batch = (jnp.ones(shape=(1, input_dim)), jnp.zeros(shape=(1, output_dim)))\n",
    "\n",
    "# Define loss function (Mean Squared Error)\n",
    "def loss_fn(params, batch):\n",
    "    inputs, targets = batch\n",
    "    outputs = model.apply(params, inputs)\n",
    "    return jnp.mean((outputs - targets) ** 2)\n",
    "\n",
    "# Define forward pass and gradient function\n",
    "forward_pass = lambda params, batch: loss_fn(params, batch)\n",
    "grad_fn = jax.grad(forward_pass)\n",
    "\n",
    "# Compute gradients\n",
    "grads = grad_fn(params, batch)\n",
    "\n",
    "# Print gradients\n",
    "print(\"Gradients:\", grads)\n",
    "# jax.tree_util.tree_map(lambda g: print(g.shape, jnp.mean(g)), grads)\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = optax.adam(learning_rate=1e-3)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "# Print optimizer state\n",
    "print(\"\\nOptimizer State:\", opt_state)\n",
    "# jax.tree_util.tree_map(lambda x: print(type(x), x.shape if hasattr(x, 'shape') else x), opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(state, inputs, targets):\n",
    "  return backward_pass(state, (inputs, targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try LoRA with simpleNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 32 \n",
    "hidden_dim = 8\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper function for creating NamedSharding\n",
    "def create_sharding(pspec):\n",
    "    return NamedSharding(mesh, pspec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mesh_sharding(pspec: PartitionSpec) -> NamedSharding:\n",
    "  return NamedSharding(mesh, pspec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import Mesh, PartitionSpec, NamedSharding\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "from jax.sharding import Mesh, PartitionSpec, NamedSharding\n",
    "from jax.lax import with_sharding_constraint\n",
    "from jax.experimental import mesh_utils\n",
    "import functools\n",
    "from functools import partial\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    hidden_dim: int\n",
    "    output_dim: int\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = with_sharding_constraint(x, mesh_sharding(PartitionSpec('data', 'model')))\n",
    "\n",
    "        x = nn.Dense(features=self.hidden_dim, \n",
    "                     kernel_init=nn.with_partitioning(nn.initializers.xavier_normal(), ('data', 'model')),\n",
    "                     use_bias=False)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=self.output_dim, \n",
    "                     use_bias=False)(x)\n",
    "        return x\n",
    "\n",
    "# Set up the device mesh\n",
    "devices = jax.devices()\n",
    "device_mesh = mesh_utils.create_device_mesh((1, 4))\n",
    "mesh = Mesh(devices=device_mesh, axis_names=('data', 'model'))\n",
    "\n",
    "print(mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = jnp.ones(shape=(1, input_dim))\n",
    "x_sharding = mesh_sharding(PartitionSpec('data', 'model')) # dimensions: (batch, length)\n",
    "sample_batch = jax.device_put(sample_batch, x_sharding)\n",
    "jax.debug.visualize_array_sharding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize function\n",
    "def init_fn(key, x, model, optimizer):\n",
    "    params = model.init(key, x)  # Initialize the model\n",
    "    state = train_state.TrainState.create(  # Create a `TrainState`\n",
    "        apply_fn=model.apply,\n",
    "        params=params['params'],\n",
    "        tx=optimizer)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN(hidden_dim, output_dim)\n",
    "optimizer = optax.adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_variables = jax.eval_shape(\n",
    "    functools.partial(\n",
    "        init_fn, \n",
    "        model=model, \n",
    "        optimizer=optimizer\n",
    "    ),\n",
    "    jax.random.PRNGKey(99),\n",
    "    sample_batch\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_sharding = nn.get_sharding(abstract_variables, mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_sharding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jit_init_fn = jax.jit(init_fn, \n",
    "                          static_argnums=(2, 3),\n",
    "                          in_shardings=(mesh_sharding(pspec=()), x_sharding),  # PRNG key and x\n",
    "                          out_shardings=state_sharding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialized_state = jit_init_fn(jax.random.PRNGKey(99), sample_batch, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialized_state.params['Dense_1']['kernel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_params(initialized_state.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(params, state, batch):\n",
    "  input_images, labels = batch\n",
    "    \n",
    "  # call forward pass function.\n",
    "  logits = state.apply_fn({\"params\": params}, input_images)\n",
    "\n",
    "  # compute loss\n",
    "  loss = optax.squared_error(logits, labels)\n",
    "  loss = loss.mean()\n",
    "  return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(state, batch):\n",
    "  # create a function to compute gradients wrt to loss\n",
    "  # returned by our `forward_pass` function.\n",
    "  grad_fn = jax.value_and_grad(forward_pass, argnums=(0), has_aux=True)\n",
    "\n",
    "  # compute gradients.\n",
    "  (loss, _), grads = grad_fn(state.params, state, batch)\n",
    "\n",
    "  # apply gradients.\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "\n",
    "  return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, in_shardings=(state_sharding, x_sharding, None),\n",
    "                   out_shardings=state_sharding)\n",
    "def train_step(state, inputs, targets):\n",
    "  return backward_pass(state, (inputs, targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = (jnp.ones(shape=(1, input_dim)), jnp.zeros(shape=(1, output_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch0 = jax.device_put(batch[0], x_sharding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1 = jax.device_put(batch[1], mesh_sharding(PartitionSpec('data', None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mesh:\n",
    "    new_state = train_step(initialized_state, batch0, batch1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_params(params):\n",
    "    flat_params = flatten_dict(params)    \n",
    "    for path, param in flat_params.items():\n",
    "        # Join the path components to create a string name\n",
    "        name = \"/\".join(str(x) for x in path)\n",
    "        print(f\"Name: {name}\")\n",
    "        print(f\"Shape: {param.shape}\")\n",
    "        print(f\"dtype: {param.dtype}\")\n",
    "        print(f\"Value: {param}\")\n",
    "        # if isinstance(param, flax.core.meta.Partitioned):\n",
    "        #     array = unbox(param)\n",
    "        # else:\n",
    "        #     array = param\n",
    "        # print(jax.debug.visualize_array_sharding(array))\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import Mesh, PartitionSpec, NamedSharding\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "from jax.sharding import Mesh, PartitionSpec, NamedSharding\n",
    "from jax.lax import with_sharding_constraint\n",
    "from jax.experimental import mesh_utils\n",
    "import functools\n",
    "from functools import partial\n",
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.experimental.pjit import pjit\n",
    "from jax.sharding import Mesh, PartitionSpec as PS\n",
    "from flax import linen as nn\n",
    "from flax.training.train_state import TrainState\n",
    "import optax\n",
    "from functools import partial\n",
    "from jax.experimental import mesh_utils\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    hidden_dim: int\n",
    "    output_dim: int\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = with_sharding_constraint(x, mesh_sharding(PartitionSpec('data', 'model')))\n",
    "\n",
    "        x = nn.Dense(features=self.hidden_dim, \n",
    "                     kernel_init=nn.with_partitioning(nn.initializers.xavier_normal(), ('data', 'model')),\n",
    "                     use_bias=False)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=self.output_dim, \n",
    "                     use_bias=False)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the device mesh\n",
    "devices = jax.devices()\n",
    "device_mesh = mesh_utils.create_device_mesh((1, 4))\n",
    "mesh = Mesh(devices=device_mesh, axis_names=('data', 'model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hidden_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m SimpleNN(\u001b[43mhidden_dim\u001b[49m, output_dim)\n\u001b[1;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39madam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hidden_dim' is not defined"
     ]
    }
   ],
   "source": [
    "model = SimpleNN(hidden_dim, output_dim)\n",
    "optimizer = optax.adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class SimpleNN(nn.Module):\n",
    "    hidden_dim: int\n",
    "    output_dim: int\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(features=self.hidden_dim, use_bias=False)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=self.output_dim, use_bias=False)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JaxRNG(object):\n",
    "    \"\"\" A convenient stateful Jax RNG wrapper. Can be used to wrap RNG inside\n",
    "        pure function.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_seed(cls, seed):\n",
    "        return cls(jax.random.PRNGKey(seed))\n",
    "\n",
    "    def __init__(self, rng):\n",
    "        self.rng = rng\n",
    "\n",
    "    def __call__(self, keys=None):\n",
    "        if keys is None:\n",
    "            self.rng, split_rng = jax.random.split(self.rng)\n",
    "            return split_rng\n",
    "        elif isinstance(keys, int):\n",
    "            split_rngs = jax.random.split(self.rng, num=keys + 1)\n",
    "            self.rng = split_rngs[0]\n",
    "            return tuple(split_rngs[1:])\n",
    "        else:\n",
    "            split_rngs = jax.random.split(self.rng, num=len(keys) + 1)\n",
    "            self.rng = split_rngs[0]\n",
    "            return {key: val for key, val in zip(keys, split_rngs[1:])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_path_to_string(path, sep=None):\n",
    "    keys = []\n",
    "    for key in path:\n",
    "        if isinstance(key, jax.tree_util.SequenceKey):\n",
    "            keys.append(str(key.idx))\n",
    "        elif isinstance(key, jax.tree_util.DictKey):\n",
    "            keys.append(str(key.key))\n",
    "        elif isinstance(key, jax.tree_util.GetAttrKey):\n",
    "            keys.append(str(key.name))\n",
    "        elif isinstance(key, jax.tree_util.FlattenedIndexKey):\n",
    "            keys.append(str(key.key))\n",
    "        else:\n",
    "            keys.append(str(key))\n",
    "    if sep is None:\n",
    "        return tuple(keys)\n",
    "    return sep.join(keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def named_tree_map(f, tree, *rest, is_leaf=None, sep=None):\n",
    "    \"\"\" An extended version of jax.tree_util.tree_map, where the mapped function\n",
    "        f takes both the name (path) and the tree leaf as input.\n",
    "    \"\"\"\n",
    "    return jax.tree_util.tree_map_with_path(\n",
    "        lambda path, x, *r: f(tree_path_to_string(path, sep=sep), x, *r),\n",
    "        tree, *rest,\n",
    "        is_leaf=is_leaf\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def match_partition_rules(rules, params):\n",
    "    \"\"\" Returns a pytree of PartitionSpec according to rules. Supports handling\n",
    "        Flax TrainState and Optax optimizer state.\n",
    "    \"\"\"\n",
    "    def get_partition_spec(name, leaf):\n",
    "        if len(leaf.shape) == 0 or np.prod(leaf.shape) == 1:\n",
    "            \"\"\" Don't partition scalar values. \"\"\"\n",
    "            return PS()\n",
    "        for rule, ps in rules:\n",
    "            if re.search(rule, name) is not None:\n",
    "                return ps\n",
    "        raise ValueError(f'Partition rule not found for param: {name}')\n",
    "    return named_tree_map(get_partition_spec, params, sep='/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jax_mesh(axis_dims, names):\n",
    "    dims = [int(x) for x in axis_dims.split(',')]\n",
    "    mesh_shape = jax.numpy.arange(jax.device_count()).reshape(dims).shape\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_sharding_constraint(x, partition_specs):\n",
    "    return jax.lax.with_sharding_constraint(x, partition_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "input_dim = 32 \n",
    "hidden_dim = 8\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNNConfigurator:\n",
    "    @staticmethod\n",
    "    def get_partition_rules():\n",
    "        return (\n",
    "            # (\"params/params/Dense_0/kernel\", PS(\"data\", \"model\")),\n",
    "            ('.*', PS(None)),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def rng_keys():\n",
    "        return ('params', 'dropout', 'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainstate_from_params(params):\n",
    "    return TrainState.create(params=params, tx=optimizer, apply_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_fn(rng, input_dim=32, hidden_dim=8, output_dim=1):\n",
    "    rng_generator = JaxRNG(rng)\n",
    "    model = SimpleNN(hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "    params = model.init(\n",
    "        rng_generator(SimpleNNConfigurator.rng_keys()),\n",
    "        jnp.zeros((4, input_dim)),\n",
    "    )\n",
    "    return TrainState.create(params=params, tx=optimizer, apply_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(train_state, rng, batch):\n",
    "    rng_generator = JaxRNG(rng)\n",
    "    batch = jax.lax.with_sharding_constraint(batch, PS(('data', 'model')))\n",
    "    \n",
    "    def loss_and_accuracy(params):\n",
    "        pred = model.apply(params, batch['input'],\n",
    "            rngs=rng_generator(SimpleNNConfigurator.rng_keys()),\n",
    "        )\n",
    "        loss = optax.squared_error(pred, batch['target'])\n",
    "        loss = loss.mean()\n",
    "        return loss\n",
    "        \n",
    "    grad_fn = jax.grad(loss_and_accuracy)\n",
    "    grads = grad_fn(train_state.params)\n",
    "    train_state = train_state.apply_gradients(grads=grads)\n",
    "    return train_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = JaxRNG(0)\n",
    "model = SimpleNN(hidden_dim=hidden_dim, output_dim=output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adam(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_shape(fun: Callable, *args, **kwargs):\n",
    "# evaluating ``fun(*args, **kwargs)``.\n",
    "\n",
    "train_state_shapes = jax.eval_shape(init_fn, jax.random.PRNGKey(99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_state_partition = match_partition_rules(\n",
    "    SimpleNNConfigurator.get_partition_rules(), train_state_shapes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_named_sharding(partition_spec):\n",
    "    return NamedSharding(mesh, partition_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_state_named_sharding = jax.tree.map(create_named_sharding, train_state_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'Dense_0': {'kernel': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))},\n",
       "  'Dense_1': {'kernel': NamedSharding(mesh=Mesh('data': 1, 'model': 4), spec=PartitionSpec(None,))}}}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_state_named_sharding.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharded_init_fn = jax.jit(\n",
    "    init_fn,\n",
    "    in_shardings=NamedSharding(mesh, PS()),\n",
    "    # out_shardings=train_state_partition, out_shardings is optional in jax.jit, GSPMD will figure it out.\n",
    "    static_argnums=(1, 2, 3)\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharded_create_trainstate_from_params = jax.jit(\n",
    "    create_trainstate_from_params,\n",
    "    in_shardings=(train_state_partition.params, ),\n",
    "    # out_shardings=train_state_partition,\n",
    "    donate_argnums=(0, ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharded_train_step = jax.jit(\n",
    "    train_step,\n",
    "    in_shardings=(train_state_named_sharding, NamedSharding(mesh, PS()), NamedSharding(mesh, PS())),\n",
    "    # out_shardings=(train_state_partition),\n",
    "    donate_argnums=(0, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_mesh = mesh_utils.create_device_mesh((1, 4))\n",
    "mesh = Mesh(devices=device_mesh, axis_names=('data', 'model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainState(step=Array(1, dtype=int32, weak_type=True), apply_fn=None, params={'params': {'Dense_0': {'kernel': Array([[ 9.58651379e-02, -5.77238090e-02,  8.71751755e-02,\n",
      "         2.06664905e-01, -1.54080182e-01,  3.74663025e-01,\n",
      "         1.21506892e-01, -1.27760872e-01],\n",
      "       [-1.65383801e-01,  9.92371961e-02, -6.61521703e-02,\n",
      "         2.75267810e-01,  3.34123582e-01,  1.94328249e-01,\n",
      "        -2.11523082e-02,  7.46464133e-02],\n",
      "       [ 6.08712770e-02, -1.41109407e-01, -8.76355544e-02,\n",
      "         2.75042266e-01, -2.81154457e-02, -1.16207547e-01,\n",
      "        -2.79863566e-01,  2.71764964e-01],\n",
      "       [ 1.61056504e-01, -9.62803438e-02,  3.30342233e-01,\n",
      "        -3.67344916e-01,  4.07360569e-02, -1.63870811e-01,\n",
      "        -1.81345776e-01,  4.84794006e-03],\n",
      "       [-9.79540274e-02,  1.76957369e-01,  3.66404593e-01,\n",
      "        -1.52017111e-02,  5.93522601e-02,  9.57616698e-03,\n",
      "         9.57029611e-02, -1.66373268e-01],\n",
      "       [ 7.82506913e-02,  2.19656542e-01,  4.44784090e-02,\n",
      "         3.93143147e-02,  1.70469340e-02,  1.64118275e-01,\n",
      "         1.50957823e-01, -2.46063903e-01],\n",
      "       [ 6.97808936e-02, -2.00764596e-01, -2.46128127e-01,\n",
      "         2.44701609e-01, -1.63323041e-02,  8.06982070e-03,\n",
      "        -2.96295464e-01,  1.10650219e-01],\n",
      "       [ 1.70194536e-01, -2.68165339e-02, -1.08412966e-01,\n",
      "        -5.80477640e-02, -3.49546336e-02,  4.79427613e-02,\n",
      "        -1.16684221e-01,  7.38132671e-02],\n",
      "       [-4.86243144e-02, -1.33826360e-01,  6.28300086e-02,\n",
      "         4.83388342e-02,  9.65549275e-02,  1.33059949e-01,\n",
      "         8.96632764e-03,  1.22078434e-01],\n",
      "       [-2.09511127e-02,  1.14838257e-01,  1.73385859e-01,\n",
      "         4.35407571e-02,  6.99398294e-02,  4.18988615e-02,\n",
      "         5.56956753e-02,  2.09704898e-02],\n",
      "       [ 6.89942855e-03,  1.03794876e-02, -1.20036125e-01,\n",
      "        -5.71607538e-02,  3.24923933e-01, -2.40171030e-02,\n",
      "         1.20772272e-02,  2.81197757e-01],\n",
      "       [-5.77472746e-02,  9.17002857e-02, -8.06393474e-02,\n",
      "        -2.31510893e-01,  7.36167058e-02,  2.92208940e-01,\n",
      "        -2.48447686e-01,  2.94839412e-01],\n",
      "       [-2.46368811e-01, -1.23847269e-01,  1.14577357e-02,\n",
      "        -1.43822134e-01,  2.46667802e-01, -1.40820205e-01,\n",
      "        -1.27565535e-02,  3.62484932e-01],\n",
      "       [-3.97865325e-02, -3.91432345e-02, -2.32120633e-01,\n",
      "        -2.80889124e-01,  1.74350828e-01, -3.55541334e-02,\n",
      "        -1.30411461e-01, -3.93446647e-02],\n",
      "       [-8.70884433e-02, -1.71413273e-03,  1.10669822e-01,\n",
      "        -1.50006041e-01,  9.38230529e-02, -3.07864338e-01,\n",
      "         2.42998004e-02,  5.59182875e-02],\n",
      "       [ 2.65604615e-01,  1.33880109e-01,  6.30205823e-03,\n",
      "        -2.38418728e-01,  2.27939278e-01, -2.98713803e-01,\n",
      "         2.56680310e-01,  3.87004346e-01],\n",
      "       [ 3.10103238e-01,  4.48181741e-02,  1.55153796e-01,\n",
      "         9.16200206e-02,  2.46349856e-01, -2.24562716e-02,\n",
      "        -1.14756174e-01, -4.45334688e-02],\n",
      "       [-1.79921836e-01, -9.40382257e-02, -1.13400564e-01,\n",
      "        -5.20668216e-02, -7.54378363e-02,  2.28565812e-01,\n",
      "         5.85835390e-02, -1.31987274e-01],\n",
      "       [-2.51752557e-04,  1.65260211e-01, -3.18870455e-01,\n",
      "        -1.91721722e-01,  1.18839145e-01, -2.09571630e-01,\n",
      "         5.47039583e-02,  1.93867907e-01],\n",
      "       [ 3.12005460e-01, -1.32446453e-01, -1.62940502e-01,\n",
      "         1.56320974e-01, -2.46608019e-01, -2.96641830e-02,\n",
      "        -3.43344151e-03,  2.97642291e-01],\n",
      "       [-1.67611614e-01, -7.98916630e-03, -3.96195846e-03,\n",
      "        -3.23131114e-01, -1.30249396e-01, -2.54046649e-01,\n",
      "        -7.49458894e-02, -1.31732047e-01],\n",
      "       [ 1.61187902e-01, -1.75606951e-01,  1.65372223e-01,\n",
      "         1.31592259e-01,  2.41059288e-01, -5.31785935e-02,\n",
      "        -9.44162831e-02,  2.10415259e-01],\n",
      "       [ 3.30500543e-01, -2.42979169e-01, -2.07141772e-01,\n",
      "        -2.35203832e-01, -2.09055617e-01, -7.46918470e-02,\n",
      "        -2.75038974e-03, -8.07918012e-02],\n",
      "       [-1.48183778e-01,  3.31676781e-01,  3.84180427e-01,\n",
      "         2.00133651e-01, -3.59694690e-01,  1.35231599e-01,\n",
      "         7.27689490e-02,  2.10119769e-01],\n",
      "       [ 1.34801697e-02, -1.36472210e-01, -1.11252077e-01,\n",
      "        -2.17809439e-01, -7.47935101e-02,  6.40343577e-02,\n",
      "         3.52699533e-02, -5.63642047e-02],\n",
      "       [-2.13968411e-01,  3.98997813e-01,  3.86152640e-02,\n",
      "         1.36926323e-01,  3.91404063e-01,  1.16414286e-01,\n",
      "        -9.56563726e-02,  1.15589812e-01],\n",
      "       [ 1.78841531e-01,  2.58336008e-01, -3.64756942e-01,\n",
      "         3.86025757e-02,  2.00053871e-01, -3.94380875e-02,\n",
      "         1.38307437e-01, -5.07196784e-02],\n",
      "       [-2.80744046e-01,  1.55669257e-01,  1.36660233e-01,\n",
      "         2.53176242e-01,  2.70596951e-01,  6.63685575e-02,\n",
      "         8.58785734e-02,  3.19406986e-01],\n",
      "       [ 9.09672454e-02,  2.30683252e-01, -3.23533028e-01,\n",
      "         2.83477128e-01,  4.71560173e-02, -4.11243457e-03,\n",
      "        -1.45658376e-02,  1.37909018e-02],\n",
      "       [ 1.09404594e-01, -2.29936000e-02, -9.60382521e-02,\n",
      "        -1.63592317e-03,  8.21953565e-02, -2.28100926e-01,\n",
      "        -1.44134134e-01,  2.87574083e-01],\n",
      "       [-2.13150844e-01, -2.15589717e-01,  2.70242803e-02,\n",
      "        -4.37777303e-02,  2.43651435e-01,  1.66691601e-01,\n",
      "        -1.03152141e-01,  3.25182348e-01],\n",
      "       [ 1.12266302e-01,  1.03202395e-01,  1.84591889e-01,\n",
      "        -7.64375255e-02, -1.08447969e-01,  1.70533508e-02,\n",
      "         1.92491814e-01, -2.37895682e-01]], dtype=float32)}, 'Dense_1': {'kernel': Array([[-0.12892883],\n",
      "       [ 0.48477697],\n",
      "       [ 0.30282158],\n",
      "       [ 0.36617175],\n",
      "       [ 0.15251277],\n",
      "       [-0.42674887],\n",
      "       [-0.05488507],\n",
      "       [ 0.11635382]], dtype=float32)}}}, tx=GradientTransformationExtraArgs(init=<function chain.<locals>.init_fn at 0x7f1b0c59f6d0>, update=<function chain.<locals>.update_fn at 0x7f1a8c209ab0>), opt_state=(ScaleByAdamState(count=Array(1, dtype=int32), mu={'params': {'Dense_0': {'kernel': Array([[-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734],\n",
      "       [-0.02382812,  0.09023438,  0.        ,  0.        ,  0.02851563,\n",
      "        -0.07929688,  0.        ,  0.02177734]], dtype=float32)}, 'Dense_1': {'kernel': Array([[0.09795997],\n",
      "       [0.1329792 ],\n",
      "       [0.        ],\n",
      "       [0.        ],\n",
      "       [0.40819302],\n",
      "       [0.00450534],\n",
      "       [0.        ],\n",
      "       [0.51165694]], dtype=float32)}}}, nu={'params': {'Dense_0': {'kernel': Array([[5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05],\n",
      "       [5.6777957e-05, 8.1422430e-04, 0.0000000e+00, 0.0000000e+00,\n",
      "        8.1314094e-05, 6.2879949e-04, 0.0000000e+00, 4.7425274e-05]],      dtype=float32)}, 'Dense_1': {'kernel': Array([[9.5961551e-04],\n",
      "       [1.7683469e-03],\n",
      "       [0.0000000e+00],\n",
      "       [0.0000000e+00],\n",
      "       [1.6662154e-02],\n",
      "       [2.0298103e-06],\n",
      "       [0.0000000e+00],\n",
      "       [2.6179284e-02]], dtype=float32)}}}), EmptyState()))\n"
     ]
    }
   ],
   "source": [
    "with mesh:\n",
    "    train_state = sharded_init_fn(jax.random.PRNGKey(99))\n",
    "    batch = {\n",
    "        'input': jnp.ones((32, input_dim)),\n",
    "        'target': jnp.zeros((32, 1), dtype=jnp.int32),\n",
    "    }\n",
    "    sharded_rng = jax.random.PRNGKey(99)\n",
    "    train_state = sharded_train_step(\n",
    "        train_state, sharded_rng, batch\n",
    "    )\n",
    "    print(train_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_state"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
